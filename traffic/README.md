# Observations

- Changing activation functions:

ReLu tend to outperform sigmoid.

- Increasing number of nodes in layer:

High numbers of sigmoid nodes tends to decrease accuracy.

# Dimensions we can experiment with

- Increase total number of layers
- Change type of layers (Convolution, Pooling, Dense, etc.)
- Change loss function
- Change activation functions
- Change dropout rates
- Change pooling techniques

# Types of layers

- Dense: Each node connects to every other node in the previous layer
- Convolution: Convoluting images to extract features
- Pooling: Reducing size of images (already done)
